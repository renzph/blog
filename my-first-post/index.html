<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=robots content="noodp">
<meta http-equiv=x-ua-compatible content="IE=edge, chrome=1">
<title>Autoregressive models, compression and metrics - luffer overflow</title><meta name=Description content="This is My New Hugo Site"><meta property="og:title" content="Autoregressive models, compression and metrics">
<meta property="og:description" content="Introduction Autoregressive models are quite popular today, especially since the OpenAI released its GPT models. The core idea of such a model is to iteratively predict parts of an object based on some already known context. In this post I want to show how this type of model connects to information theory, compression and metrics. Nothing I present is new, but when I talk to people they often are not aware of all these things.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://blog.renzph.at/my-first-post/"><meta property="og:image" content="https://blog.renzph.at/logo.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-08-17T16:29:26+02:00">
<meta property="article:modified_time" content="2021-08-17T16:29:26+02:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://blog.renzph.at/logo.png">
<meta name=twitter:title content="Autoregressive models, compression and metrics">
<meta name=twitter:description content="Introduction Autoregressive models are quite popular today, especially since the OpenAI released its GPT models. The core idea of such a model is to iteratively predict parts of an object based on some already known context. In this post I want to show how this type of model connects to information theory, compression and metrics. Nothing I present is new, but when I talk to people they often are not aware of all these things.">
<meta name=application-name content="LoveIt">
<meta name=apple-mobile-web-app-title content="LoveIt"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://blog.renzph.at/my-first-post/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Autoregressive models, compression and metrics","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/blog.renzph.at\/my-first-post\/"},"genre":"posts","wordcount":1825,"url":"https:\/\/blog.renzph.at\/my-first-post\/","datePublished":"2021-08-17T16:29:26+02:00","dateModified":"2021-08-17T16:29:26+02:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Philipp Renz"},"description":""}</script></head>
<body header-desktop header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':'auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark')&&document.body.setAttribute('theme','dark')</script>
<div id=mask></div><div class=wrapper><header class=desktop id=header-desktop>
<div class=header-wrapper>
<div class=header-title>
<a href=/ title="luffer overflow"></a>
</div>
<div class=menu>
<div class=menu-inner><a class=menu-item href=/posts/> Posts </a><a class=menu-item href=/tags/> Tags </a><a class=menu-item href=/categories/> Categories </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-desktop>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
</a>
</div>
</div>
</div>
</header><header class=mobile id=header-mobile>
<div class=header-container>
<div class=header-wrapper>
<div class=header-title>
<a href=/ title="luffer overflow"></a>
</div>
<div class=menu-toggle id=menu-toggle-mobile>
<span></span><span></span><span></span>
</div>
</div>
<div class=menu id=menu-mobile><div class=search-wrapper>
<div class="search mobile" id=search-mobile>
<input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-mobile>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</div>
<a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>
Cancel
</a>
</div><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
</a></div>
</div>
</header>
<div class="search-dropdown desktop">
<div id=search-dropdown-desktop></div>
</div>
<div class="search-dropdown mobile">
<div id=search-dropdown-mobile></div>
</div>
<main class=main>
<div class=container><div class=toc id=toc-auto>
<h2 class=toc-title>Contents</h2>
<div class=toc-content id=toc-content-auto></div>
</div><article class="page single"><h1 class="single-title animated flipInX">Autoregressive models, compression and metrics</h1><div class=post-meta>
<div class=post-meta-line><span class=post-author><a href=hi.com title=Author rel=" author" class=author><i class="fas fa-user-circle fa-fw"></i>Philipp Renz</a></span></div>
<div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2021-08-17>2021-08-17</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;1825 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;9 minutes&nbsp;</div>
</div><div class="details toc" id=toc-static kept>
<div class="details-summary toc-title">
<span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span>
</div>
<div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents>
<ul>
<li><a href=#introduction>Introduction</a></li>
<li><a href=#autoregressive-probability-modelling>Autoregressive probability modelling</a></li>
<li><a href=#compression>Compression</a></li>
<li><a href=#using-machine-learning-for-context>Using machine learning for context</a></li>
<li><a href=#entropy>Entropy</a></li>
<li><a href=#cross-entropy>Cross entropy</a></li>
<li><a href=#kl-divergence>KL-divergence</a></li>
</ul>
</nav></div>
</div><div class=content id=content><h2 id=introduction>Introduction</h2>
<p>Autoregressive models are quite popular today, especially since the OpenAI released its GPT models.
The core idea of such a model is to iteratively predict parts of an object based on some already known context.
In this post I want to show how this type of model connects to information theory,
compression and metrics.
Nothing I present is new, but when I talk to people they often are not aware of all
these things.</p>
<h2 id=autoregressive-probability-modelling>Autoregressive probability modelling</h2>
<p>In a quite popular <a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/ target=_blank rel="noopener noreffer">blog-post</a> Andrej Karpathy showed how the relatively simple idea of predicting the next character of some text can lead to suprisingly good results.
In the years that have passed since then and a bit more compute spent on the problem the same idea led to large language models such as GPT3.
The idea is to model the probability distribution of a sequence $x_1 x_2 x_3 \dots
x_T$ using the probability chain rule:
$$p(x_1 \dots x_T) = p(x_T|x_1\dots x_{T-1}) \dots p(x_3|x_1 x_2) p
(x_2|x_1) p(x_1).$$
This basically says that one wants to predict the probability distribution of a symbol in a sequence given the preceding ones.</p>
<p>Now that we have this abstract idea we can think about how to actually implement it.
A simple model family are called <a href=https://en.wikipedia.org/wiki/N-gram target=_blank rel="noopener noreffer">n-gram models</a>.
These models only look at data chunks of size $n$ and predict the last element based on the $n-1$ preceding elements.
For small enough $n$ one can build a table listing all the probabilities.
A 1-gram model, or unigram model, would make the following estimation
$$p(x_1 \dots x_T) = p(x_T) \dots p(x_3) p(x_2) p(x_1).$$
This predicts the next character of a text according to it&rsquo;s frequency in the
data. Arguably this model will not give too good results.
Using longer context would probably already yield better results. For example a 3-gram model would do the following
$$p(x_1 \dots x_T) = p(x_T|x_{T-2}x_{T-1}) \dots p(x_3|x_2 x_1) p(x_2|x_1) p(x_1).$$</p>
<p>The probabilities are estimated by counting the n-grams in the data.</p>
<p>These models are limited by the length of their context.
When using longer contexts (large $n$) it becomes unfeasible to store
the probabilities in tables. For example lets assume that each
element in a sequence represents a word and we are using a 30-gram
model and build our table using the first half of Alice in Wonderland.
Given that its highly unlikely that a sequence of 30 words occurs more
than once we&rsquo;ll get a table with a lot of 30-grams all with a count
one. When trying to apply this 30-grams to the second half of the book
we&rsquo;ll find out that it is rather useless as the second half is not covered by them.</p>
<p>Here is where machine-learning comes in.
Recurrent neural networks and Transformers can be used to make use of larger contexts.
Instead of building a table such models are tuned
minimize a loss function that measures model quality by adapting model parameters.
It is not obvious which function should be chosen for this task.</p>
<p>All we know is that it would be good to have a function that is minimal if and only if the true probabilities are reported.
Such a function is also called a strictly proper scoring rule as elaborated on <a href=https://en.wikipedia.org/wiki/Scoring_rule target=_blank rel="noopener noreffer">in this article</a>.
In practice the logarithmic scoring-rule is often used and we&rsquo;ll concentrate on it
as it has an information theoretical interpretation that we&rsquo;ll get to later.
This loss function can be written as
$$\mathcal L(\theta) = \sum_{i=1}^N \sum_{i=1}^T -\log p(x_i|x_1\dots x_{i-1}).$$
This also corresponds to the negative log-likelihood:
$$
\begin{aligned} \mathcal L(\theta) &= \sum_{i=1}^T -\log p(x_i|x_1\dots x_{i-1}) \\
&= - \log \prod_{i=1}^T p(x_i|x_1\dots x_{i-1})\\
&= - \log p(x_1 \dots x_T) \end{aligned}
$$</p>
<h2 id=compression>Compression</h2>
<p>We&rsquo;ll now look at how this relates to compression of text.
This can be explained by the basics of information theory put forward by Claude Shannon in <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6773024" target=_blank rel="noopener noreffer">A mathematical theory of communication</a>.
A core question treated in this paper is how to efficiently encode
text. ASCII for example encodes each character using one byte.
But one could save capacity by encoding less frequent symbols
using shorter codes.
Huffman came of with <a href=https://en.wikipedia.org/wiki/Huffman_coding target=_blank rel="noopener noreffer">Huffman coding</a>, a way to compute an optimal encoding based on the expected symbol frequencies.
This image gives a good intuition of the method:</p>
<figure><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Huffman_coding_visualisation.svg/1280px-Huffman_coding_visualisation.svg.png alt="Huffman coding (Source)" width=70%><figcaption>
<p>Huffman coding (<a href=https://en.wikipedia.org/wiki/Huffman_coding#/media/File:Huffman_coding_visualisation.svg>Source</a>)</p>
</figcaption>
</figure>
<p>In this example there are six different characters.
In a simple encoding we would need three bits to encode all the characters.</p>
<table>
<thead>
<tr>
<th>Char</th>
<th>Code</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>000</td>
</tr>
<tr>
<td>B</td>
<td>001</td>
</tr>
<tr>
<td>C</td>
<td>010</td>
</tr>
<tr>
<td>D</td>
<td>011</td>
</tr>
<tr>
<td>E</td>
<td>100</td>
</tr>
<tr>
<td>_</td>
<td>101</td>
</tr>
</tbody>
</table>
<p>Thus on average we would need threes bits per character (BPC) to encode the given text. The table below shows the Huffman encoding from the figure above:</p>
<table>
<thead>
<tr>
<th>Char</th>
<th>Code</th>
<th>Code length</th>
<th>p</th>
</tr>
</thead>
<tbody>
<tr>
<td>_</td>
<td>00</td>
<td>2</td>
<td>0.22</td>
</tr>
<tr>
<td>D</td>
<td>01</td>
<td>2</td>
<td>0.22</td>
</tr>
<tr>
<td>A</td>
<td>10</td>
<td>2</td>
<td>0.24</td>
</tr>
<tr>
<td>E</td>
<td>110</td>
<td>3</td>
<td>0.15</td>
</tr>
<tr>
<td>C</td>
<td>1110</td>
<td>4</td>
<td>0.04</td>
</tr>
<tr>
<td>B</td>
<td>1111</td>
<td>4</td>
<td>0.13</td>
</tr>
</tbody>
</table>
<p>Using these values we can compute the average BPC:</p>
<p>$$ E[l(s)] = \sum_{s \in \mathcal{S}} p(s) l(s) =
\underbrace{0.22 \cdot 2}_{-} +
\underbrace{0.22 \cdot 2}_{D} +
\underbrace{0.24 \cdot 2}_{A} +
\underbrace{0.15 \cdot 3}_{E} +
\underbrace{0.04 \cdot 4}_{C} +
\underbrace{0.13 \cdot 4}_{B} = 2.5 BPC $$</p>
<p>So this encoding would save us half a bit per encoded character in contrast to the constant length encoding.</p>
<p>Using this encoding one can encode text into a bitstring.
This bitstring can be decoded by reading bits until a code for a letter is obtained.
This process is unique as the encoding is a <a href=https://en.wikipedia.org/wiki/Prefix_code target=_blank rel="noopener noreffer">prefix code</a>.
The uniqueness is easier to understand if one visualizes the tree during decoding.
One starts at the root node and follows the tree according to the read bits until one hits a leaf node. Hitting a leaf node signals that a new letter starts.</p>
<h2 id=using-machine-learning-for-context>Using machine learning for context</h2>
<p>Above we modelled the probabilities using the letter frequencies.
What is apparent is that we can only reduce
the BPC because some characters are more likely than others.
It&rsquo;s not hard to see that the more concentrated the probabilities are,
the less BPC will be required.
In the extreme case where we are sure of the next character we can give that character the encoding <code>0</code> and we know that we only will need one bit per character.
Using the one-gram model above we unfortunately will not get these concentrated probabilities.
If we used more context to predict the frequency we could make more confident prediction<strong>s</strong>. The bold <code>s</code> in the preceding sentence for example could probably be predicted with high confidence.</p>
<p>Instead of just computing a static encoding we&rsquo;ll now look at how to use a machine
learning model to use more context, obtain different probability estimates for each
position in the text and use them to compute an adaptive code.
For example we could use an LSTM for compression using the following steps:</p>
<ol>
<li>Predict probabilities by LSTM given context.</li>
<li>Build the &ldquo;Huffman-tree&rdquo;</li>
<li>Encode the current character according to it</li>
<li>Add next character to context and go back to step 1.</li>
</ol>
<p>Decompressing works analogously but instead of a sequence of characters
we consume a sequence of bits and build up the character sequence that
we feed into the LSTM. We can get the code for each step using the probability
estimates obtained from the LSTM.
An toy example implementing this can be found <a href=... rel>here</a>.</p>
<h2 id=entropy>Entropy</h2>
<p>Let&rsquo;s next look at a Huffman code based on letter frequencies which are powers of two:</p>
<table>
<thead>
<tr>
<th>Char</th>
<th>Code</th>
<th>$p$</th>
<th>$-\log_2(p)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>0</td>
<td>$ \frac{1}{2} $</td>
<td>1</td>
</tr>
<tr>
<td>B</td>
<td>10</td>
<td>$ \frac{1}{4} $</td>
<td>2</td>
</tr>
<tr>
<td>C</td>
<td>110</td>
<td>$ \frac{1}{8} $</td>
<td>3</td>
</tr>
<tr>
<td>D</td>
<td>111</td>
<td>$ \frac{1}{8} $</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>We can see here that we can express the code length of a letter by the negative
logarithm of its probability.
We can write the expected BPC as a function of the probabilities
$$H(p) = \sum_i p_i (-\log_2 p_i). $$
This quantity is called the entropy of a probability distribution.
This expression gives a lower bound on the expected BPC as
shown in the <a href=https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem target=_blank rel="noopener noreffer">Shannon&rsquo;s source-coding theorem</a>.
In this case the expected BPC is equal to the entropy,
but let&rsquo;s say that the probability tree is changed an at
$p_A=0.51$ and the others $P_{BCD}=0.49$. This will decrease the
entropy, but the Huffman code will stay the same as we can&rsquo;t make the
code for A any shorter.</p>
<h2 id=cross-entropy>Cross entropy</h2>
<p>The entropy gives us a lower bound on the BPC we need to encode
the text. But this assumes that the code used is created based on the
probability distribution that generated the data.
Estimation of the probability however only results in a imperfect approximation
$q$ of the true distribution $p$.
The so called cross-entropy gives a lower bound on the BPC for data generated by $p$ and codes derived using $q$:
$$H(p,q) = - \sum_i p_i \log_2 q_i. $$
The negative logarithm again represents the code length for a character
over which a weighted sum is taken according to the probability that characters actually appear in the text.</p>
<p>Now if we go back to our ML example we have a similar situation.
We have an imperfect estimate $q_i$ at each time step and need to
measure its usefulness for compression we need to evaluate it using the
probability of the real data.
Now the problem with that is that we don&rsquo;t actually have the true
probability.
Thus we need to estimate it using a sample text $x_1\dots x_N$ which yields
$$H(p,q) = -E_{x_j \sim p}[\log_2 q_{x_j}] = \frac{1}{N} \sum_{j=1}^N \log_2 q_{x_j}.$$
Thus we get an estimate of how useful our predictions are for
compression.</p>
<h2 id=kl-divergence>KL-divergence</h2>
<p>Now one could ask oneself what the lower barrier for the cross-entropy
is and if one thinks about it for a bit it should be obvious that you
can&rsquo;t go lower than the entropy, as you should get the best result when your code
is based on the real probabilities.
Manipulating the expression for the cross entropy a bit we get
$$
\begin{aligned} H(p,q) &= - \sum_i p_i (\log_2 q_i + \log_2 p_i - \log_2 p_i) \\
&= - \sum_i p_i \log_2 p_i - \sum_i p_i (\log_2 q_i - \log_2 p_i) \\
&= H(p) + \sum_i p_i (\log_2 p_i - \log_2 q_i) \\
&= H(p) + \sum_i p_i \frac{\log_2 p_i}{\log_2 q_i} \\
&= H(p) + D_{KL}(p||q)\end{aligned}
$$
The last term is call the KL-Divergence from $p$ of $q$ which is in
fact always positive. So we get the result that the cross entropy is
always at least the entropy of $p$ with an additional term.</p>
<p>So how can we interpret this term?
The above can be written as
$$H(p,q) = H(p) + \sum_i p_i (-\log_2 q_i - (-\log_2 p_i)).$$
We write this in such a weird way because as mentioned above
$-\log q_i$ gives us the code length. Thus the KL-divergence basically
just computes the differences of code lengths for each character and
computes a weighted sum over them according to the actual data
distribution.</p>
<p>Now given that we cannot change $H(p)$ anyway we might also could
use the KL-divergence as a loss function instead of the cross entropy.</p>
</div><div class=post-footer id=post-footer>
<div class=post-info>
<div class=post-info-line>
<div class=post-info-mod>
<span>Updated on 2021-08-17</span>
</div>
<div class=post-info-license></div>
</div>
<div class=post-info-line>
<div class=post-info-md><span>
<a class=link-to-markdown href=/my-first-post/index.md target=_blank>Read Markdown</a>
</span></div>
<div class=post-info-share>
<span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://blog.renzph.at/my-first-post/ data-title="Autoregressive models, compression and metrics" data-via=xxxx><i class="fab fa-twitter fa-fw"></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://blog.renzph.at/my-first-post/><i class="fab fa-facebook-square fa-fw"></i></a><a href=javascript:void(0); title="Share on WhatsApp" data-sharer=whatsapp data-url=https://blog.renzph.at/my-first-post/ data-title="Autoregressive models, compression and metrics" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://blog.renzph.at/my-first-post/ data-title="Autoregressive models, compression and metrics"><i data-svg-src=/lib/simple-icons/icons/line.min.svg></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://blog.renzph.at/my-first-post/ data-title="Autoregressive models, compression and metrics"><i class="fab fa-weibo fa-fw"></i></a><a href=javascript:void(0); title="Share on Myspace" data-sharer=myspace data-url=https://blog.renzph.at/my-first-post/ data-title="Autoregressive models, compression and metrics" data-description><i data-svg-src=/lib/simple-icons/icons/myspace.min.svg></i></a><a href=javascript:void(0); title="Share on Blogger" data-sharer=blogger data-url=https://blog.renzph.at/my-first-post/ data-title="Autoregressive models, compression and metrics" data-description><i class="fab fa-blogger fa-fw"></i></a><a href=javascript:void(0); title="Share on Evernote" data-sharer=evernote data-url=https://blog.renzph.at/my-first-post/ data-title="Autoregressive models, compression and metrics"><i class="fab fa-evernote fa-fw"></i></a></span>
</div>
</div>
</div>
<div class=post-info-more>
<section class=post-tags></section>
<section>
<span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span>
</section>
</div>
<div class=post-nav></div>
</div>
<div id=comments></div></article></div>
</main><footer class=footer>
<div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.87.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
</div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2019 - 2021</span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div>
</div>
</footer></div>
<div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top">
<i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments">
<i class="fas fa-comment fa-fw"></i>
</a>
</div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script type=text/javascript src=/lib/smooth-scroll/smooth-scroll.min.js></script><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/lunr/lunr.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js></script><script type=text/javascript src=/lib/katex/auto-render.min.js></script><script type=text/javascript src=/lib/katex/copy-tex.min.js></script><script type=text/javascript src=/lib/cookieconsent/cookieconsent.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},cookieconsent:{content:{dismiss:"Got it!",link:"Learn more",message:"This website uses Cookies to improve your experience."},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script></body>
</html>