<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>luffer overflow</title><link>https://blog.renzph.at/</link><description>This is My New Hugo Site</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>renz.ph@gmail.com (Philipp Renz)</managingEditor><webMaster>renz.ph@gmail.com (Philipp Renz)</webMaster><lastBuildDate>Tue, 17 Aug 2021 16:29:26 +0200</lastBuildDate><atom:link href="https://blog.renzph.at/index.xml" rel="self" type="application/rss+xml"/><item><title>Autoregressive models, compression and metrics</title><link>https://blog.renzph.at/my-first-post/</link><pubDate>Tue, 17 Aug 2021 16:29:26 +0200</pubDate><author>Author</author><guid>https://blog.renzph.at/my-first-post/</guid><description>Introduction Autoregressive models are quite popular today, especially since the OpenAI released its GPT models. The core idea of such a model is to iteratively predict parts of an object based on some already known context. In this post I want to show how this type of model connects to information theory, compression and metrics. Nothing I present is new, but when I talk to people they often are not aware of all these things.</description></item></channel></rss>